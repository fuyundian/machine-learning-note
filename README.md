# 机器学习小白笔记

# 第一节、框架：关键机器学习术语

什么是（监督式）机器学习？简而言之，如下所示：

- 机器学习系统学习如何组合输入以对从未见过的数据生成有用的预测。

我们来探索一下机器学习的基本术语。

## 标签

**标签**是指我们要预测的内容，即简单线性回归中的 `y` 变量。标签可以是小麦的未来价格、图片中显示的动物类型、音频剪辑的含义，也可以是其他任何信息。

## 特性

特征是输入变量，即简单线性回归中的 `x` 变量。一个简单的机器学习项目可能会使用单个功能，而更复杂的机器学习项目可以使用数百万个功能，如下所示：

$$
𝑥1,𝑥2,...𝑥𝑁
$$

在垃圾邮件检测器示例中，这些功能可能包括：

- 电子邮件文字中的字词
- 发件人的地址
- 发送电子邮件的时间
- 电子邮件中包含词组“一种奇怪的技巧”。

## 示例

**样本**是指数据的特定实例：**x**。（我们将 **x** 显示为粗体，表示它是一个矢量。）我们将示例分为两类：

- 有标签样本
- 无标签样本

**有标签样本**同时包含特征和标签。具体来说：


 labeled examples: {features, label}: (x, y)


使用有标签样本来**训练**模型。在我们的垃圾邮件检测器示例中，有标签样本是指用户明确标记为“垃圾邮件”或“不是垃圾邮件”的个别电子邮件。

例如，下表显示了从包含加利福尼亚州房价信息的[数据集](https://developers.google.cn/machine-learning/crash-course/california-housing-data-description?hl=zh-cn)中获取的 5 个有标签样本：

| homeMedianAge（功能） | 会议室总数（设施） | 卧室总数（设施） | medianHouseValue（标签） |
| --- | --- | --- | --- |
| 15 | 5612 | 1283 | 66900 |
| 19 | 7650 | 1901 | 80100 |
| 17 | 720 | 174 | 85700 |
| 14 | 1501 | 337 | 73400 |
| 20 | 1454 | 326 | 65500 |

**无标签样本**包含特征，但不包含标签。具体来说：

 unlabeled examples: {features, ?}: (x, ?)


以下是来自同一住房数据集的 3 个无标签样本（不包括 `medianHouseValue`）：

| homeMedianAge（功能） | 会议室总数（设施） | 卧室总数（设施） |
| --- | --- | --- |
| 42 | 1686 | 361 |
| 34 | 1226 | 180 |
| 33 | 1077 | 271 |

使用有标签样本训练模型后，我们便会使用该模型来预测无标签样本的标签。在垃圾邮件检测器中，无标签样本是用户尚未添加标签的新电子邮件。

## 模型

模型定义了特征和标签之间的关系。例如，垃圾内容检测模型可能会将某些功能与“垃圾内容”紧密关联。我们重点介绍模型生命周期的两个阶段：

- **训练**是指创建或**学习**模型。也就是说，您向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。
- **推断**表示将经过训练的模型应用于无标签样本。也就是说，使用经过训练的模型做出有用的预测 (`y'`)。例如，在推理期间，您可以针对新的无标签样本预测 `medianHouseValue`。

## 回归与分类

**回归**模型可预测连续值。例如，回归模型做出的预测可回答如下问题：

- 加利福尼亚州一栋房子的价值是多少？
- 用户点击此广告的可能性有多大？

**分类**模型可预测离散值。例如，分类模型做出的预测可回答如下问题：

- 指定的电子邮件是垃圾邮件还是非垃圾邮件？
- 这是狗、猫还是仓鼠的图片？

## 监督式学习

查看以下选项。

假设您想开发一种监督式机器学习模型来预测指定的电子邮件是“垃圾邮件”还是“非垃圾邮件”。下列哪些陈述是正确的？

我们将使用无标签样本来训练模型。

我们将使用

**有标签**

样本来训练模型。然后，我们可以针对无标签样本运行经过训练的模型，以推断无标签的电子邮件是垃圾邮件还是非垃圾邮件。

**请重试。**

主题标头中的字词会成为良好的标签。

主题标头中的字词可能具有出色的特征，但并不适合用作标签。

**请重试。**

未标记为“垃圾邮件”或“不是垃圾邮件”的电子邮件是无标签样本。

由于我们的标签由值“垃圾邮件”和“非垃圾邮件”组成，因此任何未标记为垃圾邮件或非垃圾邮件的电子邮件都是无标签样本。

**正确答案共有 2 个，您目前选中了 1 个。**

应用于某些示例的标签可能不可靠。

当然可以。请务必检查数据的可靠性。此数据集的标签可能来自将特定电子邮件标记为垃圾邮件的电子邮件用户。由于大多数用户不会将每封可疑的电子邮件都标记为垃圾邮件，因此我们可能不知道电子邮件是否为垃圾邮件。此外，垃圾内容发布者可能会故意提供错误的标签来误导我们的模型。

**正确答案共有 2 个，您目前选中了 2 个。**

## 功能和标签

查看以下选项。

假设一家在线鞋店希望创建一种监督式机器学习模型，为用户提供个性化的鞋子推荐服务。也就是说，该模型会向小马推荐某些鞋子，而向小杰推荐另外一些鞋子。系统将使用过去的用户行为数据生成训练数据。下列哪些陈述是正确的？

用户喜欢的鞋子是一种实用的标签。

“喜好”不是可观察且可量化的指标。我们所能做的就是搜索可爱的代理指标。

**请重试。**

用户点击了鞋子的描述，这是个有用的标签。

用户可能只是想详细了解自己喜欢的鞋子。因此，“点击次数”是一种可观测、可量化的指标，可以用作合适的训练标签。由于我们的训练数据源自既往用户行为，因此我们的标签需要源自与用户偏好密切相关的客观行为。

**正确答案共有 2 个，您目前选中了 2 个。**

“鞋类美妆”是一项实用功能。

良好的特征具体且可量化。美观性太过模糊，无法用作有用的特征。 美观程度可能是某些具体特征（例如样式和颜色）的综合考量。样式和颜色都比美观性更好。

**请重试。**

“鞋码”是一项实用功能。

“鞋码”是一种可量化的信号，可能对用户是否会喜欢推荐的鞋子有很大的影响。例如，如果马蒂穿 9 号鞋，则不建议建议尺码 7 的鞋。

**正确答案共有 2 个，您目前选中了 1 个。**

# 第二节、深入了解机器学习：线性回归

长久以来，人们都知道板球（昆虫物种）在天气炎热的日子里会比在更冷的日子里鸣叫。数十年来，专业和业余科学家一直在编制每分钟的鸣叫声和温度方面的数据。Ruth 姨妈给您送上生日祝福，送上一份生日礼物，邀请您学习一个模型来预测这种关系。 您想利用这些数据来探索这种关系。

首先，通过绘制数据的方式检查数据：

![https://developers.google.cn/static/machine-learning/crash-course/images/CricketPoints.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/CricketPoints.svg?hl=zh-cn)

**图 1. 每分钟的鸣叫声与温度（以摄氏度为单位）。**

如您所料，该图表显示了温度随着鸣叫声次数的增加而上升。 鸣叫声与温度之间的关系是线性关系吗？可以，您可以绘制一条直线来大致说明这种关系：

![https://developers.google.cn/static/machine-learning/crash-course/images/CricketLine.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/CricketLine.svg?hl=zh-cn)

**图 2. 一种线性关系。**

没错，虽然这个直线并未穿过每一个点，但清晰地显示了鸣叫声和温度之间的关系。使用直线的等式，您可以写出这种关系，如下所示：

$$
𝑦=𝑚𝑥+𝑏
$$

其中：

- 𝑦是摄氏度，即我们正在尝试预测的值。
- 𝑚是直线的斜率。
- 𝑥是每分钟的鸣叫声次数，即输入特征的值。
- 𝑏是 y 轴截距。

按照机器学习的惯例，您为模型算式的过程会略有不同：

$$
𝑦′=𝑏+𝑤_1𝑥_1
$$

其中：

- 𝑦′是预测的[标签](https://developers.google.cn/machine-learning/crash-course/framing/ml-terminology?hl=zh-cn#labels)（理想输出）。
- 𝑏是偏差（y 轴截距），有时称为 。
- 𝑤1是特征 1 的权重。权重与传统线方程中的“斜率”  概念相同。
- 𝑥1是一项[功能](https://developers.google.cn/machine-learning/crash-course/framing/ml-terminology?hl=zh-cn#features)（已知输入）。



如需根据新的每分钟的鸣叫声值 𝑦′ 推断温度，只需将 𝑥_1 值替换为此模型即可。 𝑦′ 𝑥_1𝑥_1

虽然此模型仅使用一个特征，但更复杂的模型可能依赖于多个特征，每个特征都有单独的权重（𝑤1、 𝑤2等）。例如，一个依赖于三个特征的模型可能如下所示：

$$
𝑦′=𝑏+𝑤_1𝑥_1+𝑤_2𝑥_2+𝑤_3𝑥_3
$$

# **第三节、深入了解机器学习：训练和损失**

训练模型只需从有标签样本中学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过检查许多示例并尝试找到将损失降至最低的模型来构建模型；此过程称为经验风险最小化。

损失是错误预测的惩罚。也就是说，**损失**是一个表示模型在单个样本上的预测质量的数字。如果模型的预测完全准确，则损失为零，否则损失会更大。训练模型的目的是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，图 3 左侧显示的是高损失模型，右侧显示的是低损失模型。对于该图，请注意以下几点：

- 箭头表示损失。
- 蓝线表示预测。

![https://developers.google.cn/machine-learning/crash-course/images/LossSideBySide.png?hl=zh-cn](https://developers.google.cn/machine-learning/crash-course/images/LossSideBySide.png?hl=zh-cn)

**图 3. 左侧模型中的损失较高；右侧模型中的损失较低。**

请注意，左侧曲线图中的箭头比右侧曲线图中的箭头长得多。显然，相较于左侧曲线图中的线条，右侧曲线图中的预测模型要好得多。

您可能想知道是否可以创建数学函数（损失函数），以有意义的方式汇总各个损失。

### 平方损失函数：一种常用的损失函数

我们在此探讨的线性回归模型使用一种称为**平方损失函数**（也称为 **L2 损失**）的损失函数。单个样本的平方损失如下：

```
  = the square of the difference between the label and the prediction
  标签和预测之间差异的平方
  = (observation - prediction(x))2
  = (y - y')2
```

**均方误差** (**MSE**) 是指整个数据集中每个样本的平均平方损失。如需计算 MSE，请先计算各个样本的所有平方损失之和，然后除以样本数量：

$$
𝑀𝑆𝐸=\frac{1}{𝑁}\sum_{(𝑥,𝑦)\isin𝐷}(x−y_1)^2
$$

其中：

- (𝑥,𝑦)是一个示例，其中
    - 𝑥是模型用于进行预测的特征集（例如，每分钟的鸣叫次数、年龄、性别）。
    - 𝑦是示例标签（例如，温度）。
- 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛(𝑥)是权重和偏差与一组特征的组合𝑥 。
- 𝐷是一个包含许多有标签样本（成对）的数据集。(𝑥,𝑦)
- 𝑁是 𝐷中的样本数。

虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情况的最佳损失函数。

## 均方误差

请参考以下两个图表：

![Untitled](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%99%BD%E7%AC%94%E8%AE%B0%20d37c050debad4e87bc9e5cf1702b99c6/Untitled.png)

| 序号 | 预测值 x,y  | 输入属性 x,y  | 计算均值平方 |
| --- | --- | --- | --- |
| 1 | 1,2.5 | 1,2.5 | 0^2 |
| 2 | 2,3 | 2,4 | 1^2 |
| 3 | 3,3.5 | 3,3.5 | 0^2 |
| 4 | 4,4 | 4,3 | 1^2 |
| 5 | 4.5,4.5 | 4.5,4.5 | 0^2 |
| 6 | 6,5 | 6,4 | 1^2 |
| 7 | 7,5.5 | 7,5.5 | 0^2 |
| 8 | 8,6 | 8,7 | 1^2 |
| 9 | 9,6.5 | 9,6.5 | 0^2 |
| 10 | 10,7 | 10,7 | 0^2 |

![Untitled](%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%99%BD%E7%AC%94%E8%AE%B0%20d37c050debad4e87bc9e5cf1702b99c6/Untitled%201.png)

| 序号 | 预测值 x,y  | 输入属性 x,y  | 计算均值平方 |
| --- | --- | --- | --- |
| 1 | 1,2.5 | 1,2.5 | 0^2 |
| 2 | 2,3 | 2,3 | 0^2 |
| 3 | 3,3.5 | 3,3.5 | 0^2 |
| 4 | 4,4 | 4,6 | 2^2 |
| 5 | 4.5,4.5 | 4.5,4.5 | 0^2 |
| 6 | 6,5 | 6,5 | 0^2 |
| 7 | 7,5.5 | 7,5.5 | 0^2 |
| 8 | 8,6 | 8,4 | 2^2 |
| 9 | 9,6.5 | 9,6.5 | 0^2 |
| 10 | 10,7 | 10,7 | 0^2 |

·

查看以下选项。

以上各个图表中显示的两个数据集的均方误差 (MSE) 较高？

右侧的数据集。

线上的 8 个示例导致的总损失为 0。不过，尽管只有两个点在线外，但这两个点离线的距离依然是左图中离群点的 2 倍。平方损失进一步放大这些差异，因此 2 的偏移量产生的损失是 1 的偏移量的四倍

$$
𝑀𝑆𝐸=\frac{0^2+0^2+0^2+2^2+0^2+0^2+0^2+2^2+0^2+0^2}{10}=0.8
$$

**正确答案。**

左侧的数据集。

线上的 6 个示例发生的总损失为 0。四个不在线的样本并未远离离线，因此，即便使它们的平方变平方仍会产生较低的值：

$$
𝑀𝑆𝐸=\frac{0^2+1^2+0^2+1^2+0^2+1^2+0^2+1^2+0^2+0^2}{10}=0.4
$$

**请重试。**

# 第四节、**降低损失：迭代方法**

为了训练模型，我们需要一种降低模型损失的好方法。迭代方法是一种广泛用于减少损失的方法，而且简单高效，就像爬山一样

介绍了损失的概念。在本单元中，您将了解机器学习模型如何以迭代方式减少损失。

迭代学习可能会让您想到[“Hot and Cold”](http://www.howcast.com/videos/258352-how-to-play-hot-and-cold/)这种寻找隐藏物体（如顶针）的儿童游戏。在该游戏中，“隐藏的物品”是最佳模型。您可以大胆猜测（“ 𝑤_1 的值为 0”），然后等待系统告诉您损失是多少。然后，您再尝试另一种猜测（“ 𝑤_1 的值为 0.5”），看看损失是什么。事实上，如果您玩得正确，通常会逐渐变热。游戏真正的诀窍在于尽可能高效地找到最佳模型。

下图展示了机器学习算法用于训练模型的迭代试错过程：

![https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentDiagram.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentDiagram.svg?hl=zh-cn)

**图 1. 一种用于训练模型的迭代方法。**

我们将在整个机器学习速成课程中使用相同的迭代方法，详细说明各种复杂功能，尤其是在标记为“模型（预测函数）”的暴风雨云中。迭代策略在机器学习中很常见，主要是因为它们可以很好地扩展到大型数据集。

“模型”将一个或多个特征作为输入，并返回一项预测作为输出。为简单起见，假设一个模型接受一个特征 (𝑥_1) 并返回一个预测 (𝑦′)：

$$
𝑦′=𝑏+𝑤_1𝑥_1
$$

我们应该为 𝑏和 𝑤1设置哪些初始值？对于线性回归问题，事实证明起始值并不重要。我们可以选择随机值，但采用下面这些无关紧要的值：

- 𝑏= 0
- 𝑤_1= 0

假设第一个特征值是 10。将该特征值插入到预测函数中可产生：

$$
𝑦′=0+0⋅10=0
$$

图中的“计算损失”部分是模型将使用的[损失函数](https://developers.google.cn/machine-learning/crash-course/descending-into-ml/training-and-loss?hl=zh-cn)。假设我们使用平方损失函数。损失函数有两个输入值：

- 𝑦′：模型对特征 x 的预测*x*
- 𝑦：特征 x 对应的正确标签。*x*

最后，我们已经到达图中的“计算参数更新”部分。机器学习系统会在这里检查损失函数的值，并为 𝑏 和 𝑤1生成新值。现在，我们先假设这个神秘的框会产生新值，然后机器学习系统会根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。学习过程会持续迭代，直到该算法发现损失可能最低的模型参数。通常，您可以反复迭代，直到整体损失不再发生变化或变化速度至少变化得非常缓慢。这时候，我们可以说该模型已**收敛**。

**要点**：
训练机器学习模型时，首先对权重和偏差进行初始猜测，然后以迭代方式调整这些猜测，直到学习出损失可能最低的权重和偏差。

**迭代方法**：

- 是一种通过重复执行相同或相似的步骤来逐步逼近目标或解决方案的方法。
- 不断重复一个过程，根据每次的结果进行调整和改进，直到达到期望的结果或满足特定的条件。

# 第五节、降低损失：梯度下降法

迭代方法图（[图 1](https://developers.google.cn/machine-learning/crash-course/reducing-loss/an-iterative-approach?hl=zh-cn#ml-block-diagram)）包含一个标题为“计算参数更新”的华而不实的绿色方框。我们现在要用更实质的方法代替这种精明的算法。

假设我们有时间和计算资源来计算 𝑤_1的所有可能值的损失。对于我们一直在研究的那类回归问题，产生的损失与 𝑤_1 的图表始终是凸形。换句话说，图表将始终是碗状图，如下所示：

![https://developers.google.cn/static/machine-learning/crash-course/images/convex.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/convex.svg?hl=zh-cn)

**图 2. 回归问题产生的损失与权重图呈凸形。**

凸形问题只有一个最小值；即只有一个位置的斜率正好为 0。这个最小值就是损失函数收敛的位置。

通过计算整个数据集内每个可能的 𝑤1值的损失函数来寻找收敛点的效率非常低下。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一阶段是为 𝑤1选择一个起始值（起点）。起点并不重要；因此，许多算法直接将 𝑤1 设置为 0 或选择随机值。在下图中，我们选择了一个略大于 0 的起点：

![https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentStartingPoint.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentStartingPoint.svg?hl=zh-cn)

**图 3. 梯度下降法的起点。**

然后，梯度下降法算法会计算损失曲线在起点的梯度。在图 3 中，损失的梯度等于曲线的[导数](https://wikipedia.org/wiki/Differential_calculus#Derivative)（斜率），可以告诉您哪个方向是“更暖”还是“冷”。当有多个权重时，**梯度**是偏导数相对于权重的矢量。

**点击加号图标可详细了解偏导数和梯度。**

请注意，渐变是一种矢量，因此具有以下两个特征：

- 方向
- 震级

梯度始终指向损失函数中增长最显著的方向。梯度下降法朝着负梯度方向前进，以便尽快减少损失。

![https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg?hl=zh-cn)

**图 4. 梯度下降法依赖于负梯度。**

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分添加到起点，如下图所示：

![https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentGradientStep.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/GradientDescentGradientStep.svg?hl=zh-cn)

**图 5. 梯度步长将我们移动到损失曲线上的下一个点。**

然后，梯度下降法会重复此过程，接近最小值。

**注意** ：执行梯度下降法时，我们会泛化上述过程，以同时调整所有模型参数。例如，为了找到 𝑤1 和偏差 𝑏的最优值，我们会同时计算 𝑤1 和 𝑏的梯度。接下来，我们根据 𝑤1和 𝑏 各自的梯度修改它们的值。然后重复上述步骤，直到达到最小损失。

咱们想象这样一个场景：

假设你在一个黑漆漆的迷宫里，你的目标是找到迷宫的出口，而每走一步都会消耗你的体力（这就相当于损失）。

梯度下降法就像是你有一种特殊的能力，能感觉到哪个方向是最有可能通向出口的（就像能感知梯度的方向），但是这个感觉不是完全准确的。

迭代方法呢，就是你每次根据这个感觉走几步，然后停下来看看自己的位置（相当于计算当前的损失）。

比如说，你感觉前面是出口的方向，你就走几步。走了几步之后，你再重新判断一下自己是不是离出口更近了，如果更近了，就继续按照这个方向再走几步；如果发现好像没那么近，那就调整一下方向再走。

就这样，你不断地根据自己对方向的感觉（梯度）走几步，然后看看结果（损失），再调整方向继续走，直到最终走出迷宫（损失降到最低）。

这就是梯度下降法和迭代方法结合起来的通俗解释，**不断尝试、调整，朝着降低损失的方向前进**。

**运算值**

那咱们假设一个简单的例子。假设我们要优化的函数:

$$
f(x) =x^2+3x+1
$$

要找到这个函数的最小值，也就是让 f**( x)** 最小的**x** 值。我们先随便选一个初始值，假设**x=2** ，学习率 **a= 0.1** 。

首先，对于我们给定的优化函数，计算其导数可以使用基本的求导公式。

**导数运算**

对于的导数

1. x的平方导数，计算公式如下：

$$
x^n = nx^{n-1}

$$

1. 对于常数项的导数是 0 。

$$
计算出f(x) =x^2+3x+1 导数， f(x) = 2x^{2-1} + 3*(x^{1-1}) +常数项的导数是  0,所以最终导数计算转换为=2x + 3
$$

我们计算函数在 **x = 2** 处的导数：

$$
f'(x) = 2x +3
$$

所以

$$
f'(x) = 2* 2 +3 =7
$$

然后根据梯度下降法的公式：

$$
x = 2- 0.1 * 7 = 1.3
$$

接下来，我们以新的  再次进行计算。

$$
f'(1.3) = 2* 1.3 +3 =5.6
$$

然后根据梯度下降法的公式：

$$
x = 1.3- 0.1 * 5.6 = 0.74
$$

我们继续这个过程，不断更新**x**的值，直到**x**的变化很小，或者达到我们满意的精度。

**梯度下降法**：

- 一种用于寻找函数最小值的优化算法。
- 想象在一个山坡上，通过感知当前位置的坡度（梯度），朝着下坡的方向一步步走，以找到山的最低点（函数的最小值）。

# 第六节、降低损失 (Reducing Loss)：学习速率

如上所述，梯度矢量同时具有方向和大小。梯度下降法算法会将梯度乘以称为“学习速率”（有时也称为“步长”）的标量，以确定下一个点。例如，如果梯度大小为 2.5 且学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的下一个点。

**超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习程序员都会花费大量时间来调整学习速率。如果您选择的学习速率过小，则学习将会花费太长时间：

![https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateTooSmall.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateTooSmall.svg?hl=zh-cn)

**图 6. 学习速率过小。**

相反，如果您指定的学习速率过大，则下一个点将永远在井底随意弹跳，就像量子力学实验大错一样：

![https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateTooLarge.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateTooLarge.svg?hl=zh-cn)

**图 7. 学习速率过高。**

每个回归问题都存在一个[金发姑娘](https://wikipedia.org/wiki/Goldilocks_principle)学习速率。“金发姑娘”值与损失函数的平坦程度有关。如果您知道损失函数的梯度较小，则可以放心地尝试较大的学习速率，以抵消小的梯度，从而产生较大的步长。

![https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateJustRight.svg?hl=zh-cn](https://developers.google.cn/static/machine-learning/crash-course/images/LearningRateJustRight.svg?hl=zh-cn)

**图 8. 学习速率恰到好处。**

一维空间中的理想学习速率是 1/𝑓″(𝑥) （f(x) 对 x 的二次导数的逆）。

二维或多维空间中的理想学习速率是[海森矩阵](https://wikipedia.org/wiki/Hessian_matrix)（二阶偏导数的矩阵）的倒数。

广义凸函数的情况则更加复杂。

**学习速率**：

- 在梯度下降法和相关算法中，控制每次参数更新的步长。
- 学习速率过大可能导致跳过最优解，过小则可能导致收敛速度过慢。

# 第七节、降低损失 (Reducing Loss)：随机梯度下降法

在梯度下降法中，**批次**是用于在单次训练迭代中计算梯度的一组样本。到目前为止，我们假定该批次是整个数据集。在 Google 的规模下，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含大量特征。因此，一个批次可能非常庞大。超大的批量也可能会导致单次迭代就可能需要很长时间才能完成计算。

具有随机抽样样本的大型数据集可能包含冗余数据。事实上，随着批次大小的增加，冗余的可能性会越来越高。一些冗余可能有助于消除嘈杂的梯度，但与大批量相比，大量的批量往往具有更高的预测价值。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从数据集内随机选择样本，我们可以从小得多的样本中估算出大的平均值（尽管会有噪声）。 **随机梯度下降法** (**SGD**) 将这种想法运用到极致，它每次迭代只使用一个样本（批次大小为 1）。如果有足够的迭代，SGD 可以正常工作，但噪声非常嘈杂。术语“随机”表示构成每个批次的一个样本是随机选择的。

**小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批次通常包含 10 到 1,000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的噪声数，但仍然比全批量更高效。

为简化说明，我们重点关注单个特征的梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。

随机梯度下降法，，举个例子：

假设您想要减肥，而您的体重取决于每天摄入的卡路里和运动量。我们把这个关系看作一个复杂的数学函数。

现在，随机梯度下降法就像是您每天的尝试。

比如说，第一天您随机选择了只吃蔬菜（这就相当于随机选择了一个样本），然后发现体重没怎么下降，这就相当于计算出了这个选择下体重变化的梯度方向不太好。

第二天您又随机选择了跑两公里（又是一个随机样本），发现体重下降了一些，这就是一个相对较好的梯度方向。

然后接下来的每一天，您都随机选择一种饮食和运动的组合（随机样本），根据体重的变化（梯度）来调整第二天的选择。可能有时候效果不好，有时候效果好，但总体上您会逐渐找到能让体重持续下降的最佳饮食和运动组合。

**随机梯度下降法（SGD）**：

- 梯度下降法的一种变体。
- 每次迭代不是基于全部数据计算梯度，而是随机选取一个或一小批数据来计算梯度并更新参数。
- 计算效率高，适用于大规模数据，但由于梯度的随机性，可能导致参数更新不稳定。

**全量迭代**：

- 在每次迭代中使用全部数据来计算梯度和更新参数。
- 能更准确地反映数据的整体特征，但计算量大，对于大规模数据可能不实用。

**批量迭代**：

- 每次迭代使用数据的一个子集（称为一批）来计算梯度和更新参数。
- 是介于全量迭代和随机梯度下降法之间的一种方式，在一定程度上平衡了计算效率和梯度的准确性

# 第八节、降低损失 (Reducing Loss)：检查您的理解情况

## 检查您的理解情况：批次大小

了解以下选项。

在大型数据集上执行梯度下降法时，以下哪个批量大小可能更高效？

小批量，甚至是包含一个样本的批量 (SGD)。

令人惊讶的是，对小批量甚至包含一个样本的批量执行梯度下降法通常比对全批量更高效。毕竟，计算一个样本的梯度比计算数百万个样本的梯度要低得多。 为确保获得良好的代表性样本，该算法在每次迭代时都会抽取另一个随机的小批次（或一个批次的批次）。

**正确答案。**

完整批次。

从全批次计算梯度的效率并不高。也就是说，与非常大的全批次相比，小批次计算梯度的效率通常更高（且准确度一样高）。

**请重试。**

# 第九节、使用 TensorFlow 的起始步骤：编程练习

**预计用时**：60 分钟

通过机器学习速成课程的学习，您将通过在 tf.keras 中对模型进行编码来实际运用机器学习概念。您将使用 **Colab** 作为编程环境。Colab 是 Google 的 [Jupyter 笔记本](https://jupyter.org/)版本。与 Jupyter 笔记本一样，Colab 提供了一个将文本、代码、图形和程序输出组合在一起的交互式 Python 编程环境。

## NumPy 和 Pandas

使用 tf.keras 时，您至少需要对以下两个开源 Python 库有所了解：

- [NumPy](https://numpy.org/) - 简化对数组的表示和执行线性代数运算。
  - 
- [pandas](https://pandas.pydata.org/)，它提供了一种表示内存中数据集的简单方法。

如果您不熟悉 NumPy 或 Pandas，请先执行以下两项 Colab 练习：

1. [NumPy UltraQuick 教程](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=numpy_tf2-colab&hl=zh-cn) Colab 练习，提供本课程所需的所有 NumPy 信息。
    1. 考试1.任务 创建线性数据集
        1. 您的目标是创建一个由单个特征和标签组成的简单数据集，如下所示：
            1. 将从 6 到 20（含）的整数序列分配给名为 的 NumPy 数组`feature`
            2. 将 15 个值分配给 NumPy 数组，其名称`label`

            ```python
            label = (3)(feature) + 4
            feature = ?
            print(feature)
            label = (3)(feature) + 4
            print(label)
            ```

           解题代码如下：

            ```python
            import numpy as np 
            feature = np.arange(6,20)
            print(feature)
            label = (3 * feature) +  4
            print(label)
            ```

    2. 考试任务 2：向数据集添加一些噪声
        1. 噪声
            1. 为了使数据集更加真实，请在已创建的标签数组的每个元素中插入一些随机噪声。更准确地说，通过添加 -2 和 +2 之间的不同随机浮点值来修改分配给标签的每个值。
            2. 不要随机数。相反，创建一个与标签具有相同维度的噪声数组。

            ```python
            noise = ?    # write your code here
            print(noise)
            label = ?    # write your code here
            print(label)
            ```

           解题代码如下：

            ```python
            import numpy as np 
            noise = (np.random.random([6]) * 4) - 2
            print(noise)
            label = noise + 0.1
            print(label)
            ```

2. [Pandas UltraQuick 教程](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=pandas_tf2-colab&hl=zh-cn) Colab 练习，提供本课程所需的所有 Pandas 信息。
    1. 任务 1：创建 DataFrame
        1. 请执行下列操作：
            1. 创建一个 3x4（3 行 x 4 列）的 pandas DataFrame，其中的列名为`Eleanor`、 `Chidi`、`Tahani`和`Jason`。使用 0 到 100 之间的随机整数（包括 0 和 100）填充 DataFrame 中的 12 个单元格中的每一个。
            2. 输出以下内容：
                - 整个 DataFrame
                - `Eleanor`

                  该列第 1 行单元格中的值

            3. 创建第五列，名为，其中填充了和`Janet`的逐行总和。`TahaniJason`
            4. 为了完成此任务，了解 NumPy UltraQuick 教程中涵盖的 NumPy 基础知识会有所帮助。

## 使用 tf.keras 进行线性回归

在了解 NumPy 和 Pandas 中的技能后，请执行以下两项 Colab 练习，探索 tf.keras 中的线性回归和超参数调节：

1. [使用合成数据进行线性回归](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression_synthetic_tf2-colab&hl=zh-cn) Colab 练习：使用玩具数据集探索线性回归。
2. [使用实际数据集进行线性回归](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_a_real_dataset.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression_real_tf2-colab&hl=zh-cn) Colab 练习，它会引导您在真实数据集上进行各种分析。